---
title: "[Machine Learning Design Patterns] Chapter 4: 모델 학습 디자인 패턴"
categories:
  - Machine Learning Design Patterns
tags:
  - Study
  - Machine Learning
  - MLOps
gallery:
gallery:
  - url: /assets/images/머신러닝디자인패턴.jpg
    image_path: /assets/images/머신러닝디자인패턴.jpg
    alt: "placeholder image 1"
    title: "머신러닝 디자인패턴"
---

![머신러닝디자인패턴](https://user-images.githubusercontent.com/104043279/164156828-b2e17094-7cdc-456e-84c2-321fada963de.jpg)      
           
## CHAPTER5 탄력성 디자인 패턴  
   
ml모델을 학습시킨 후에는 일반적으로 프로덕션 환경에 배포하고, 수신된 요청에 대한 응답으로 예측을 수행한다. 프로덕션 환경에 배포된 소프트웨어는 사람의 개입이 거의 없이 실행을 유지해야 하므로, 탄력성을 갖추는 것이 무엇보다 중요하다. 스테이트리스 함수를 사용하면 서버 코드를 단순화하고 확장성을 높일 수 있다.   
     
---      
      
### 5.1 디자인 패턴 16: 스테이트리스 서빙 함수stateless serving function   
프로덕션 ml 시스템에서 스테이트리스 서빙 함수 디자인 패턴을 사용해 초당 수천에서 수백만 개의 예측 요청을 __동기식으로__ 처리할 수 있다.   
    
```    
< 스테이트리스 함수 >   
입력에 의해 출력이 결정되는 함수   
ex1) def stateless_fn(x):   
        return 3*x + 15    
     
가중치와 편향이 상수로 저장되는 불변 객체 또한 스테이트리스 함수이다.
ex2) class Stateless:   
        def __init__(self):   
            self.weight = 3    
            self.bias = 15   
        def __call__(self, x):    
            return self.weight*x + self.bias    
```      
    
#### 문제   
ml모델을 활용해 예측할 경우 발생할 수 있는 몇 가지 문제가 있다.    
* 임베딩 계층이 상당히 클 수 있다. 또한 계층이 많은 딥러닝 모델도 상당히 클 수 있다.   
* 아키텍처에서 predict() 메서드에 대한 호출을 하나씩 전송해야 할 때 달성할 수 있는 최소 지연 시간에 제한이 생긴다.   
* 데이터 과학자가 선택한 프로그래밍 언어는 파이썬일지라도 모델 추론의 경우 달라질 수 있다. 다른 언어를 선호하는 개발자가 작성한 프로그램이나, 안드로이드/iOS와 같은 모바일 플랫폼에서도 호출될 가능성이 높다.   
* 학습에 가장 효과적인 모델 입출력 형식이 정작 실제 사용에 있어서는 사용자 친화적이지 않을 수 있다.   
    
#### 솔루션   
솔루션은 다음 단계로 구성된다.   
1. __모델의 수학적 핵심을 파악하고 프로그래밍 언어에 구애받지 않는 형식으로 모델을 내보낸다.__   
  : 케라스의 경우, model.save() 사용
2. __프로덕션 시스템에서 모델의 _순방향_ 계산으로만 구성된 공식이 스테이트리스 함수로 복원된다.__   
  : 내보낸 모델은 프로덕션 시스템에서 프로토콜 버퍼와 다른 연관 파일들로부터 복원된다. (-> 특정 모델 서명을 준수하는 입출력 변수명과 데이터 유형을 가진 스테이트리스 함수)   
3. __스테이트리스 함수는 REST 엔드포인트를 제공하는 프레임워크에 배포된다.__  
  : 웹 애플리케이션/Google App Engine/Heroku/AWS 람다/.. 등과 같은 서버리스 프레임워크에 해당 함수를 배포한다. 이 프레임워크들은 개발자가 실행해야하는 함수를 구체적으로 지정할 수 있고, 인프라 오토스케일링도 지원한다는 장점이 있다.   
    
```   
< 웹 애플리케이션 프레임워크의 장점 >   
- 오토스케일링: 낮은 지연 시간으로 초당 많은 수의 예측 요청 처리   
- 완전 관리형 서비스: 구성 요소의 관리 및 설치를 손쉽게 할 수 있도록 함  
- 언어 중립성: 코드를 여러 언어로 작성 가능   
- 강력한 생태계: 사용량 측정, 모니터링, 관리 등에 사용할 수 있는 도구가 많음   
```   
    
#### 트레이드오프와 대안   
* __다중 서명__   
적은 수의 클라이언트가 매우 비용이 많이 드는 작업을 필요로 하는 경우, 다양한 서빙 서명을 제공하고 클라이언트가 호출할 서명을 제공 프레임워크에 알리도록 하는 것이 유용하다.   
   
```   
ex)      
model.save(export_path, signatures={
          'serving_default': func1,
          'expensive_result': func2,
  })     
    
이후, 입력JSON 요청에는 원하는 모델의 엔드포인트를 선택하는 서명 이름을 포함한다.   
{    
  "signature_name": "expensive_result",
  {"instances": ...}
}   
```   
   
* __온라인 예측__    
내보낸 서빙 함수는 궁극적으로는 파일 형식이므로 기존의 ml 프레임워크가 온라인 예측을 지원하지 않더라도 온라인 예측 기능을 제공하는 데 사용할 수 있다.   
   
* __예측 라이브러리__    
REST API를 통해 호출할 수 있는 마이크로서비스로 서빙 함수를 배포하는 대신 예측 코드를 라이브러리 함수로 구현할 수 있다. 라이브러리 함수는 처음 호출될 때 내보낸 모델을 로드하고 제공된 입력으로 model.predict()를 호출한 뒤 결과를 반환한다.   
물리적 이유(네트워크 연결 없음) 또는 성능 제약으로 인해 네트워크를 통해 모델을 호출할 수 없는 경우, 라이브러리 함수가 마이크로서비스보다 낫다. 다만, 라이브러리 함수는 모델의 유지 관리 및 업데이트가 어렵다. 또한 라이브러리가 작성된 프로그래밍 언어로 제한되어 모델 활용도가 떨어진다. 모델의 업데이트 및 활용도 측면에서는 마이크로서비스 방식이 더 낫다.  
     
---     
     
### 5.2 디자인 패턴 17: 배치 서빙batch serving   
배치 서빙 디자인 패턴은 분산 데이터 처리에 일반적으로 사용되는 소프트웨어 인프라를 사용하여 한 번에 많은 인스턴스에 대한 추론을 수행한다.    
    
#### 문제   
일반적으로 예측은 요청 시 한 번에 하나씩 수행된다. 서빙 프레임워크는 5.1절에서 말한 것처럼 개별 요청을 가능한 한 빨리 동기적으로 처리하도록 설계되었다. 그러나 대량의 데이터에 대해 비동기적으로 예측을 수행해야 하는 상황도 있다. 이 경우 ml모델은 한 번에 하나의 인스턴스가 아니라 한 번에 수백만 개의 인스턴스에 대한 예측을 수행해야 한다. 한 번에 하나의 요청을 처리하도록 설계된 소프트웨어 엔드포인트를 사용하여 수백만 개의 보관단위(stock keeping unit: SKU) 또는 수십억 명의 사용자에게 응답을 보내려고 하면 ml모델이 제대로 동작하지 않게 된다.    
    
#### 솔루션   
배치 서빙 디자인 패턴은 **분산 데이터 처리 인프라**를 사용하여 많은 수의 인스턴스에서 비동기식으로 ml 추론을 수행한다.   
**분산 데이터 처리 인프라: MapReduce, Apache Spark, BigQuery, Apach Beam 등*   
      
#### 작동 원리    
스테이트리스 서빙 함수는 낮은 지연 시간이라는 장점이 있지만, 수백만 개의 항목을 가끔 또는 주기적으로 처리하는데 이용하기엔 많은 비용이 소모된다. 요청에 대한 지연 시간에 민감하지 않은 경우 분산 데이터 처리 아키텍처를 사용하여 수백만 개의 항목에 대해 ml 모델을 호출하는 것이 비용 면에서 더욱 효율적이다. 수백만 개의 항목에 대해 ml 모델을 호출하는 것은 병렬 처리 문제이기 때문이다.    
ex) 백만 개의 항목을 가져와서 각각 1000개의 항목으로 구성된 1000개의 그룹으로 나누고, 각 항목 그룹을 컴퓨터로 보낸 다음 마지막에 결합한다.: __분할 정복 기법(divided and conquer)__   
    
#### 트레이드오프와 대안   
* __배치 및 스트림 파이프라인__    
아파치 스파크 또는 아파치 빔과 같은 프레임워크는 입력에 전처리가 필요하거나 출력에 후처리가 필요한 경우, 전처리나 후처리를 SQL로 표현하기 어려운 경우에 유용하다. (ex: 입력이 이미지/오디오/비디오인 경우)    
위의 프레임워크는 또한 클라이언트 코드에 상태 유지가 필요한 경우에도 유용하다. 클라이언트가 상태를 유지해야 하는 일반적인 이유는 ml 모델에 대한 입력 중 하나가 시간 간격의 평균(time-windowed average)인 경우가 많기 때문이다. 이 경우 클라이언트 코드는 들어오는 데이터 스트림의 이동 평균을 수행하고 이동 평균을 ml 모델에 제공해야 한다.   
    
* __배치 서빙 결과 캐시__    
추론을 수행하는 ml 프레임워크가 병렬 처리를 활용할 수 있다. 배치 서빙을 사용하여 대규모의 사용자에 대한 모델 예측결과를 미리 계산한다. 그리고 이를 관계형 데이터베이스에 저장한다. 사용자가 방문하면 해당 사용자에 대한 결과를 데이터베이스에서 가져온다. 이럴 경우 매우 짧은 대기 시간을 거쳐 예측 결과를 제공할 수 있다.   
    
* __람다 아키텍처__   
온라인 서빙과 배치 서빙을 모두 지원하는 프로덕션 ml 시스템을 람다 아키텍처라고 한다. 이러한 프로덕션 ml 시스템을 사용하면 ml 실무자가 지연 시간과 처치량 간의 균형을 맞출 수 있다.     
     
---     
      
### 5.3 디자인 패턴 18: 연속 모델 평가continued model evaluation    
연속 모델 평가 디자인 패턴은 배포된 모델이 더 이상 목적에 적합하지 않을 때를 감지하고 조치를 취해야 하는 문제를 처리할 때 사용된다.   
   
#### 문제   
모델 배포 이후에는 다음과 같은 질문들이 나오게 된다. 모델이 실제 환경에서 예상대로 작동하는가? 입력 데이터에 예기치 않은 변경이 있다면 어떻게 되는가? 모델이 더 이상 정확하거나 유용한 예측을 생성하지 않는다면? 이러한 변화를 감지할 수 있을까? 이는 세상은 동적으로 변화하지만 ml 모델은 과거 데이터로부터 생성된 정적 모델이기 때문에 나오는 질문들이다. 즉, 모델이 실제 서비스 환경에 들어가면 성능이 저하되기 시작하고 예측이 점점 더 불안정해질 수 있다. 이렇게 시간이 지남에 따라 모델이 저하되는 주된 이유로는 **개념 드리프트**와 **데이터 드리프트**가 있다.   
* 개념 드리프트: 상대방의 행동 패턴이 변화하면서 기본 가정이 변경되었을 때 발생   
* 데이터 드리프트: 학습 데이터와 비교하여 모델에 제공된 데이터에 변화가 일어났을 때 발생   
     
모델 배포는 지속적인 프로세스이며 개념 드리프트, 데이터 드리프트를 해결하려면 학습 데이터셋을 업데이트하고 새로운 데이터로 모델을 다시 학습하여 예측을 개선해야 한다. 재학습 시기와 재학습 빈도를 알아내는 것은 매우 중요하다.   
모델 배포 -> 샘플의 예측 저장 -> 정답 수집 -> 모델 성능 평가 -> 지속적인 평가 과정을 거치며 모델의 예측 성능을 지속적으로 모니터링하고, 기능 저하를 식별한다.    
      
#### 트레이드오프와 대안    
* __재학습을 위한 트리거 설정__   
일반적으로 모델 성능은 시간이 지남에 따라 저하된다. 성능이 얼마나 떨어졌을 때 재학습을 할 것인지 트리거를 정해야 한다. 임곗값은 절대값 또는 성능 변화율로 설정할 수 있다. 임곗값을 선택하는 것은 체크포인트 모델과 비슷하다. 더 높고 더 민감한 임곗값을 사용하면 실제 서비스에 배포된 모델이 최신 상태로 유지되지만 빈번한 재학습 및 다른 모델들의 버전 유지 및 전환으로 인해 높은 비용이 들 수 있다. 임곗값이 낮으면 학습 비용이 감소하지만 실제 서비스되는 모델은 오래된 상태일 가능성이 높아진다.   
   
* __예정된 재학습__    
재학습 일정은 비즈니스 사용 사례, 새 데이터의 보급률, 재학습 파이프라인 실행 시간 및 비용에 따라 달라진다. 일반적으로 최적의 시간 간격은 실무자의 경험과 실험을 통해 결정된다.   
   
* __TFX를 활용한 데이터 검증__   
데이터 분포는 시간이 지남에 따라 변경될 수 있다. 이와 같은 데이터 드리프트 현상이 일어날 때 모델은 '오래된 모델'이 되고 새로운 데이터로 다시 학습해야 한다. 지속적인 평가는 배포된 모델을 모니터링하는 사후 방법을 제공하지만, 서빙하는 동안 수신되는 새 데이터를 모니터링하고 데이터 배포의 변경 사항을 선제적으로 식별하는 것도 중요하다.   
TFX는 구글에서 오픈소스로 제공하는 ml 모델을 배포하기 위한 엔드 투 엔드 플랫폼으로, TFX의 데이터 유효성 검사를 통해 데이터 검증을 수행할 수 있다. 데이터 유효성 검사 라이브러리를 사용하면 학습에 사용된 데이터 예제와 서빙 중에 수집된 데이터 예제를 비교할 수 있다. 이를 통해 학습 제공 편향이나 데이터 드리프트를 감지한다.    
   
* __재학습 간격 추정__    
데이터와 개념 드리프트가 모델에 미치는 영향을 이해하기 위한 상대적으로 저렴한 전략은 오래된 데이터만 사용하여 모델을 학습하고, 최신 데이터에서 해당 모델의 성능을 평가하는 것이다. 즉, 6개월~1년 전의 데이터를 수집하고 일반적인 모델 개발 워크플로를 통해 특징을 생성한 후, 하이퍼파라미터를 최적화하고 관련 평가 지표들을 수집한다. 이후 해당 평가 지표를 모델 예측과 비교하여 한 달 전에 수집된 최신 데이터에 대한 비교 작업을 수행한다. 현재 데이터에 비해 오래된 데이터로 학습시킨 모델의 성능이 얼마나 저하되는지 확인하고, 이를 통해 시간이 지남에 따라 모델의 성능이 저하되는 비율과 재학습이 필요한 빈도를 비교적 신뢰도 있게 추정할 수 있다.   
     
---      
      
### 5.4 디자인 패턴 19: 2단계 예측two-phase prediction   
2단계 예측 디자인 패턴은 크고 복잡한 모델을 분산된 장치에 배포해야 할 때 발생하는 성능 유지 문제를 두 단계로 분할한 케이스로 해결하는 디자인 패턴이다.   
   
#### 문제   
ml 모델을 배포할 때, 최종 사용자가 항상 신뢰할 수 있는 인터넷에 연결할 수 있는지는 보장할 수 없다. 어떤 경우 모델은 인터넷 연결을 사용하지 않는 edge 환경에 배포된다. 학습된 모델을 edge 장치에서 작동하는 형식으로 변환하기 위해 모델은 양자화(quantization) 프로세스를 거치며, 학습된 모델 가중치는 더 적은 바이트로 표현된다. 양자화 외에도 edge 장치용 모델은 메모리/프로세서 제약에 맞추기 위해 크기가 더 작을 수 있다.    
     
#### 솔루션   
* __1단계: 오프라인 모델__    
2단계 예측 디자인 패턴 중 첫 번째 모델은 인터넷 연결에 의존하지 않고 빠른 추론을 진행해야 한다. -> 모바일 기기에 로드할 수 있을만큼 충분히 작아야 한다.   
학습된 모델을 내보내는 일반적인 방식은 텐서플로우의 model.save() 메서드이지만, 이 모델은 가능한 한 작게 유지되어야 하므로 양자화 프로세스를 활용한 **텐서플로 라이트**를 사용한다.   
* __2단계: 클라우드 모델__   
클라우드 호스팅 모델은 네트워크 연결 없이 진행될 필요가 없기 때문에 전통적인 접근 방식을 따를 수 있다. -> 다양한 형태를 취하는 것이 가능.   
    
#### 트레이드오프와 대안   
* __독립 1단계 모델__    
때로는 모델의 최종 사용자가 인터넷 연결이 거의 또는 전혀 없을 수 있다. 이 경우 2단계 예측 흐름에 의존하는 대신 첫 번째 모델만으로도 자급자족할 수 있을만큼 충분히 견고하게 만들어야 한다. 오프라인 추론을 위해 설계된 더 복잡한 모델을 구축하려면, 학습 중과 학습 후에 모델의 가중치 및 기타 수학 연산을 양자화할 수 있는 도구를 사용하는 것이 가장 좋다.: __양자화 인식 학습(quantization aware training)__    
     
* __특정 사용 상황을 위한 오프라인 지원__    
애플리케이션의 특정 부분만 오프라인에서 사용할 수 있도록 하는 것도 인터넷 연결이 최소화된 사용자를 위한 방법 중 하나이다. 몇 가지 일반적인 기능을 오프라인으로 사용 설정하거나, 나중에 오프라인에서 사용할 수 있도록 ml 모델의 예측 결과를 캐싱하는 것도 여기에 포함된다. 이 접근방식에서 앱은 충분히 오프라인으로 작동하지만 연결이 다시 설정되면 전체 기능을 제공한다. 또한 기기가 오프라인일 때 사용자의 쿼리를 저장하고 연결이 회복되면 클라우드 모델로 전송하여 더 자세한 결과를 제공할 수도 있다.    
     
* __많은 예측을 실시간에 가깝게 처리__   
ml 모델의 최종 사용자가 안정적인 연결을 가지긴 하나 한 번에 수백 또는 수천 개의 예측을 수행해야 할 수도 있다. 클라우드 호스팅 모델만 있고 각 예측에 호스팅된 서비스에 대한 API 호출이 필요하다고 할 때, 예측 응답을 한 번에 가져오는 데 너무 많은 시간이 걸릴 수 있다. 모든 데이터 포인트를 클라우드 모델로 보내는 것은 비효율적이고 비용 문제도 발생한다. 이럴 경우 오프라인 모델을 통해 일차적으로 데이터를 판단하고, 통합 검증을 위해 필요하다고 생각되는 데이터들만 클라우드 모델에 보낼 수 있다. 모델이 한 번에 클라우드 모델로 전송되는 예측 요청 수를 조절하게 되는 것이다.   
     
* __오프라인 모델에 대한 지속적인 평가가 필요하다.__     
     
---      
      
### 5.5 디자인 패턴 20: 키 기반 예측keyed prediction   
키 기반 예측 디자인 패턴은 모델이 클라이언트 제공 키를 통과하는 것이 유리할 경우 사용되는 디자인 패턴이다. 여러 디자인 패턴을 확장 가능하게 구현하는 데 필요하다.   
    
#### 문제   
모델이 백만 개의 입력이 있는 파일을 수락하고 백만 개의 출력 예측이 있는 파일을 보낼 경우 어떤 일이 발생할까?  
- 입력 인스턴스와 출력 인스턴스가 1:1 관계일 때는 각 서버 노드가 이를 연속적으로 처리해야 한다. -> 확장성 문제 야기   
- 분산 데이터 처리 시스템을 사용하여 여러 시스템에 인스턴스를 보내 계산한 뒤, 결과 출력을 모두 수집하여 다시 보낸다. -> 출력 내용의 순서가 일정하지 않음  
- 온라인 서빙 시스템이 스테이트리스 서빙 함수 패턴에 설명된대로 인스턴스 배열을 수락한다. -> 온라인 서빙 핫스팟 문제 제기    
    
#### 솔루션   
클라이언트가 각 입력과 관련된 키를 제공하도록 하여, 통과 키(pass through key)를 사용한다. (a,b,c)를 입력받으면 d를 출력받는 모델일 때, 클라이언트가 모델에 (k,a,b,c)를 제공하도록 한다. **k: 고유 식별자가 있는 키*    
그러면 모델이 (k,d)를 반환하므로 클라이언트는 어떤 출력 인스턴스가 어떤 입력 인스턴스에 해당하는지 알아낼 수 있게 된다.     
* 케라스에서 통과 키를 사용하는 방법: 모델을 내보낼 때 '서빙 서명' 사용    
* 기존 모델에 키 기반 예측 추가: tf.saved_model.load()를 사용하여 모델을 로드하고 서빙 함수를 추가한 다음 다시 저장+배포한다.    
     
#### 트레이드오프와 대안   
비동기 서빙(ex.행렬곱셈)이나 지속적인 평가를 수행하는 등의 상황에서도 클라이언트가 키를 제공하도록 하는 것이 유용하다.    