---
title: "Chapter 2: 데이터 표현 디자인 패턴"
categories:
  - Machine Learning Design Patterns
tags:
  - Study
  - Machine Learning
  - MLOps
gallery:
gallery:
  - url: /assets/images/머신러닝디자인패턴.jpg
    image_path: /assets/images/머신러닝디자인패턴.jpg
    alt: "placeholder image 1"
    title: "머신러닝 디자인패턴"
---

머신러닝 모델의 핵심은 특정 유형의 데이터로만 작동하도록 정의된 '수학적' 함수다. 그러나 동시에, 머신러닝 모델은 수학적 함수에 직접 연결할 수 '없는' 데이터로도 작동할 수 있어야 한다.     
     
데이터 표현   
=============   

수치 데이터     
-------------     
대부분의 최신 대규모 머신러닝 모델은 수치 기반으로 작동하므로, 입력값이 수치로 되어 있다면 이를 변경하지 않고 모델에 전달할 수 있다.
그러나, **스케일링은 필요하다.**    
     
**1. 상당수의 ML 프레임워크는 [-1, 1] 범위 내의 수치에서 잘 작동하도록 조정된 옵티마이저를 사용하기 때문이다.**     
     
```   
  <수치를 [-1, 1] 범위로 스케일링하는 이유는?>     
  경사 하강법gradient descent 옵티마이저는 손실 함수loss function의 곡률이 증가함에 따라 수렴하는 데 더 많은 단계를 필요로 한다.    
  → 특징의 상대적인 크기가 더 크다면 미분도 큰 경향이 있어, 손실 함수의 곡률이 크면 비정상적인 가중치 업데이트로 이어지기 때문 (비정상적으로 큰 가중치 업데이트는 수렴하는 데 더 많은 단계 필요 → 계산 부하 증가)    
  데이터를 [-1, 1] 범위에서 '중앙에 배치'하면 오류 함수가 더 완만해진다. 따라서 이 범위로 스케일링된 데이터로 학습시킨 모델은 더 빨리 수렴하는 경향이 있어, 학습 속도가 빨라지거나 비용이 저렴해진다.    
  또한, [-1, 1] 범위에서 가장 높은 부동 소수점 정밀도를 얻을 수 있다.    
```   
     
**2. 일부 머신러닝 알고리즘 및 기술이 서로 다른 특징의 상대적인 크기에 매우 민감하기 때문이다.**     
     
### 스케일링 종류     
**◾ Min-Max 스케일링**     
최솟값은 -1로 변환하고 최댓값은 1로 변환하는 선형 변환     
균일하게 분산된 데이터에서 가장 잘 작동     
문제점: 최댓값과 최솟값을 학습 데이터셋 내에서 추정해야 하며, 이러한 값이 outlier일 가능성이 높다. outlier를 최댓값 또는 최솟값으로 잡으면 실제 데이터가 [-1, 1] 범위 내에서 매우 좁은 범위로 축소될 수도 있다.     
     
**◾ 클리핑**     
Min-Max 스케일링과 함께 사용한다.     
outlier값을 -1 또는 1로 처리하는 효과     
     
**◾ Z score 정규화**     
학습 데이터셋에 대해 추정한 평균과 표준편차를 사용하여 선형 스케일링       
정규분포를 따르는 데이터에서 가장 잘 작동     
[-1, 1] 범위를 벗어나는 값은 여전히 존재 (절댓값이 클수록 존재할 확률이 적다)     
     
**◾ 윈저라이징**    
학습 데이터셋의 경험적 분포를 사용하여 데이터값이 10번째 및 90번째 백분위 수(or 5번째 및 95번째 백분위수 등)에 해당하는 경계로 데이터셋 클리핑 → Min-Max 스케일링     
     
데이터가 치유쳐 균등하게 분포되지 않았거나, 종형 곡선처럼 분포되지 않는다면 스케일링 전, **비선형 변환nonlinear transform**을 적용해야 한다.     
→ 스케일링 전에 입력값의 로그를 취하거나, sigmoid 함수 적용, 또는 다항식 전개(제곱/제곱근/세제곱/세제곱근 등)     
→ 변환된 값의 분포가 균일하거나 정규분포를 따르게 됐다면 변환 함수를 잘 적용한 것     
     
     
카테고리 데이터     
-------------     
수치값으로 바꿔주어야 한다.     
*원-핫 인코딩: 변수의 독립성을 유지하면서 카테고리형 입력 변수를 매핑하는 가장 간단한 방법 (특징 벡터로 변환)*    
     
아래와 같은 경우에는 겉보기에는 수치형 데이터처럼 보이더라도 카테고리형으로 처리해야 한다.       
```   
  1) 수치 입력이 인덱스인 경우     
  2) 입력과 라벨의 관계가 연속적이지 않을 때     
  3) 수치 변수를 버킷화하는 것이 유리할 때     
  4) 라벨에 미치는 영향과 관련하여 수치 입력의 다른 값을 독립적으로 취급하려 할 때       
```   
     
       
***    
      
        
디자인 패턴 1: 특징 해시(hashed feature)     
=============     
특징 해시hashed feature 디자인 패턴은 카테고리 특징과 관련해 발생 가능한 세 가지 문제인   
     
  **1) 불완전한 어휘**     
  무작위 샘플링으로는 학습 데이터 내의 전체 어휘가 포함되지 않을 수 있음      
  **2) 카디널리티cardinality로 인한 모델 크기**     
  언어 3개, 요일 7개 같은 특징 벡터를 사용하는 게 아니라 길이가 수천에서 수백만에 이르는 특징 벡터가 존재    
  → 특징 벡터의 길이가 길면 가중치가 너무 많아서 학습 데이터가 부족해짐     
  → 모델을 학습시킬 수는 있더라도 서빙할 때는 학습모델을 저장하기 위한 상당히 큰 공간이 필요해짐      
  → 작은 장치에는 모델을 배포하지 못할 수 있음      
  **3) 콜드 스타트cold start**      
  모델이 배포된 후에도 새로운 어휘 등장할 수 있는데, 모델은 이에 대해 예측할 수 없음      
      
를 해결한다. 이 디자인 패턴은 카테고리형 특징을 그룹화하고 데이터 표현의 충돌이 가지는 트레이드오프를 인정하는데서 출발한다.   
      
작동 원리      
-------------     
특징 해시 디자인 패턴은 다음과 같은 방식으로 카테고리형 입력 변수를 표현한다.     
- 카테고리 입력을 고유한 문자열로 변환     
- 문자열에 대해 결정론적(무작위 시드나 솔트 없음) 및 이식 가능한(동일한 알고리즘을 학습 및 서빙 모두에 사용할 수 있음) 해시 알고리즘을 호출     
- 해시 결과를 원하는 버킷 수로 나누고 나머지를 취한다. (버킷화=그룹화)     
      
해시를 쓰면 손실이 발생하는 것은 사실이다. 그러나 너무 많은 변수 때문에 이를 줄여야만 하는 상황이라면, 손실이 존재하는 인코딩으로 절충해야 한다.    
     

***     
     

디자인 패턴 2: 임베딩(embeding)     
=============      
임베딩embedding은 학습 문제와 관련된 정보가 보존되는 방식으로, 높은 카디널리티 데이터를 저차원 공간에 매핑하는, 학습 가능한 데이터 표현의 일종이다.    
원ㅡ핫 인코딩은,     
```     
  1) 고려해야 할 카테고리가 수없이 많을 때(=카테고리가 큰 카디널리티를 가질 때) → 원ㅡ핫 인코딩을 쓸 경우 차원 수가 굉장히 많이 늘어난다. → 밀도가 낮은, 즉 머신러닝 알고리즘에 적합하지 않은 행렬이 생성된다.    
  2) 원ㅡ핫 인코딩은 카테고리형 변수를 독립적으로 취급한다.     
```     
위와 같은 문제점이 있고, 이 문제를 해결하기 위한 것이 바로 임베딩 디자인 패턴이다.    
     
임베딩 디자인 패턴은 고차원의 카테고리형 입력 변수를 저차원 공간의 실수 벡터로 매핑한다. 즉, 임베딩은 입력 데이터의 밀착도를 찾아낸다.    
     
임베딩은 입력 데이터의 밀착도 관계를 낮은 차원에서 표현하기 때문에, 클러스터링 기술이나 주성분 분석PCA과 같은 차원 감소 방법을 임베딩 계층으로 대체하는 것이 가능하다. 임베딩의 가중치는 기본 모델의 학습 루프에서 결정되므로, 미리 클러스터링하거나 PCA를 수행할 필요가 없다.     
      
작동 원리      
-------------      
임베딩 계층은 신경망의 또 다른 숨겨진 계층이다. 가중치는 카디널리티가 큰 차원 각각에 연결되고, 나머지 네트워크를 통해 출력으로 이어진다. 따라서 임베딩을 생성하는 가중치는 신경망의 다른 가중치와 마찬가지로 경사 하강법 프로세스를 통해 학습된다. 즉, 벡터 임베딩은 학습 데이터 입력 특징값의 가장 효율적인 저차원 표현을 나타낸다.     
따라서 학습된 임베딩을 사용하면 2개의 개별 카테고리 사이의 고유한 유사성을 추출할 수 있으며, 수치 벡터 표현이 있다면 두 카테고리형 특징 간의 유사성을 정확하게 정량화할 수 있다.     
임베딩 사용의 단점은 데이터 표현이 손상된다는 점이다. 카디널리티가 큰 표현을 저차원 표현으로 이동시키는 과정에서 정보 손실이 발생한다. 그 대가로 항목 간의 밀착도와 콘텍스트에 대한 정보를 얻을 수 있다.      
      
유의점       
-------------      
임베딩 계층의 출력 차원이 너무 작으면 많은 정보가 작은 벡터 공간에 강제로 들어가 콘텍스트 정보가 손실될 수 있다. 반면에 임베딩 차원이 너무 크면 임베딩의 특징 각각의 중요성을 잃게 된다. (원ㅡ핫 인코딩은 임베딩 차원이 극단적으로 큰 예시) 최적의 임베딩 차원은 딥러닝 계층의 뉴런의 수를 선택할 때와 마찬가지로 실험을 통해 찾아내야 한다. 그럼에도 불구하고 시간을 아끼고 싶다면,     
```      
  1. 고유한 카테고리형 원수 총수의 네제곱근을 사용     
  2. 임베딩 차원이 고유한 카테고리형 원소 총수 제곱근의 약 1.6배     
```      
위와 같은 방식으로 접근해 보는 것도 괜찮을 것이다.    
      

***     
         
          
디자인 패턴 3: 특징 교차(feature cross)     
=============      
특징 교차feature cross 디자인 패턴은 입력값의 각 조합을 별도의 특징으로 명시적으로 만들어, 모델이 입력 간의 관계를 더 빨리 학습하도록 도와준다. 특징 교차는 둘 이상의 카테고리 특징을 연결하여 이들 간의 상호작용을 반영하도록 합성된 특징이다. 신경망이나 트리와 같이 더 복잡한 모델은 자체적으로 특징 교차를 학습할 수 있지만, **특징 교차를 명시적으로 사용하면 선형 모델만 학습할 수 있다.**      
결과적으로 특징 교차는 모델 학습 속도를 높여 비용을 절감하고, 모델 복잡성을 줄여 필요한 학습 데이터를 줄일 수 있다.      
데이터가 충분하면 특징 교차 패턴을 통해 모델이 더 단순해질 수 있다. (=학습 시간은 대폭 단축되면서 성능은 거의 유사한 모델을 얻을 수 있다.)     
       
작동 원리          
-------------         
특징 교차는 특징 가공의 중요한 수단으로, 단순한 모델에 더 많은 복잡성, 더 많은 표현력, 더 많은 용량을 제공한다.       
특징 교차는 방대한 데이터에도 잘 맞는다. → 학습 데이터 내의 복잡한 관계를 학습하기 위한 하나의 전략으로 쓰인다.         
          
트레이드오프와 대안           
-------------            
**◾ 수치 특징일 때**         
특징 교차는 카테고리형 변수뿐만 아니라 수치 특징에도 적용할 수 있다. 그런데 수치 입력은 밀도가 높고 연속된 값을 사용하기 때문에, 수치 특징에 특징 교차를 사용할 때는 약간의 전처리 과정을 거친다. → 연속적인 데이터에 특징 교차를 바로 적용하지 않고 그 전에 데이터를 버킷화하여 카테고리화한다.       
          
**◾ 큰 카디널리티**             
특징 교차를 수행하면 카테고리의 카디널리티가 입력 특징의 카디널리티에 비해 몇 배로 증가하기 때문에 특징 교차는 모델 입력의 밀도를 낮춘다. → 특징 교차 결과를 임베딩 계층으로 보내서 저차원 표현을 만든다.         
          
**◾ 정규화 필요**         
특징 교차를 수행한 후에는 특징 밀도를 높이는 L1정규화 또는 과대적합을 제한하는 L2정규화와 함께 쓰는 것이 좋다. 모델은 정규화로 합성 특징에 의해 생성된 많은 외부 노이즈를 무시하고, 과대적합을 방지할 수 있다.       
           
특징 교차를 위해 결합할 특징을 선택할 때 상관관계가 높은 두 특징을 교차하는 것은 권하지 않는다. 특징 교차는 두 특징을 결합하여 정렬된 쌍을 만드는 것으로, 두 특징의 상관관계가 높으면 특징 교차의 결과물이 모델에 새로운 정보를 가져오지 않기 때문이다.       
          

***          


디자인 패턴 4: 멀티모달 입력(multimodal input)        
=============            
멀티모달 입력multimodal input 디자인 패턴은 사용 가능한 모든 데이터 표현을 연결하여, 복잡한 방식으로 표현할 수 있는 데이터 또는 다양한 유형의 데이터를 표현하는 문제를 해결한다.        
많은 모델이 특정한 입력 유형에 대해서만 정의되어 있다. (ex. Resnet-50은 이미지 이외의 입력을 처리할 수 없다.)         
       
작동 원리           
-------------          
입력 중 하나가 자유 형식 텍스트인 구조화된 데이터 모델을 학습할 때도, 수치 데이터와 달리 이미지나 텍스트는 모델에 직접 공급할 수 없다. 그래서 모델이 이해할 수 있는 방식으로 이미지 및 텍스트 입력을 표현한 다음, 이러한 입력을 다른 테이블 형식 특징과 결합해야 한다. 이런 과정을 거쳐 멀티모달 입력을 허용하는 단일 모델을 만든다.        
            
트레이드오프와 대안          
-------------           
멀티모달 입력 디자인 패턴은 동일한 모델에서 서로 다른 입력 형식을 표현하는 방법을 찾아낸다. 서로 다른 유형의 데이터를 혼합하는 것 외에도, 모델이 패턴을 더 쉽게 식별할 수 있도록 동일한 데이터를 다른 방식으로 표현할 수 있다. (ex. 별 1개에서 별 5개까지의 평점 필드: 수치/카테고리 둘 다로 취급 가능)     
### ◾ 테이블 데이터를 다양한 방식으로 표현      
수치형 or 카테고리형 둘 다로 취급 가능한 데이터를 어떤 식으로 처리할지 결정하고 실현       
### ◾ 텍스트 데이터를 다양한 방식으로 표현           
**◾임베딩**         
**◾BOW 방식**         
BOW 방식을 사용하여 텍스트를 표현할 때는, 모델에 입력된 텍스트를 글자가 아닌 단어를 담고 있는 주머니처럼 간주한다.       
텍스트의 순서를 유지하지는 않지만 모델에 보내는 각 텍스트 조각에서 특정 단어의 존재 여부를 감지 → 텍스트 입력이 1과 0의 배열로 변환되는 멀티-핫 인코딩 유형이 된다.       
BOW 배열의 각 인덱스는 고유한 각 단어에 해당한다.        
            
임베딩은 모델에 계층을 추가하고 BOW 인코딩에서 사용할 수 없는 단어 의미에 대한 정보를 제공한다. 그러나 학습을 필요로 한다.        
BOW 인코딩은 선택한 예측 작업이 데이터셋에서 작동하는지 확인하거나 빠르게 프로토타입을 만드는 데 유용하다. 다만, 단어의 순서나 의미를 고려하지 않기 때문에 이러한 요소가 예측 작업에 중요하다면 사용할 수 없다.        
BOW 인코딩은 어휘에 있는 가장 중요한 단어에 대해 강력한 신호를 제공하는 반면, 임베딩은 훨씬 더 큰 어휘 내에서 단어 간의 관계를 식별할 수 있다.     
데이터에서 더 많은 패턴을 추출하기 위해 단어 주머니와 텍스트 임베딩 표현을 결합하는 딥러닝 모델을 만들 수도 있다.        
       
### ◾ 텍스트에서 테이블 특징 추출        
텍스트에는 다양한 측면이 있는데, 단어 자체와는 관련이 없음에도 모델이 예측하고자 하는 결과와 관련이 있는 부분도 있다.(ex. 질문의 길이, 물음표의 존재 여부) 그러나 임베딩을 만들 때는 이런 요소가 해당 데이터 표현에서 손실된다. 멀티모달 입력 디자인 패턴을 사용하면 이렇게 손실된 정보를 모델로 되돌릴 수 있다.         
          
### ◾ 이미지의 멀티모달 표현           
- 이미지를 타일 구조로 표현: CNN         
- 서로 다른 이미지 표현의 결합: keras의 Concatenate 계층 사용         
어떤 이미지 표현을 사용할지, 멀티모달 표현을 사용할지 여부를 선택하는 것은 보통 이미지 데이터의 유형에 달려 있다. 일반적으로 이미지가 더 자세할수록 이미지를 타일 또는 슬라이딩윈도로 표현하는 것이 좋다. 반면 복잡한 이미지의 경우 여러 이미지 표현을 결합해야 정확도가 높아질 것이다. **이미지를 픽셀값으로 표현하면 고수준의 초점 포인트를 잘 식별할 수 있는 반면, 타일 표현은 보다 대비가 낮은 모서리와 형상을 잘 식별하게 만든다.**         
         
딥러닝 모델은 본질적으로 설명하기 어렵다. 단일 모델에서 여러 데이터 표현을 결합하면 특징이 서로 종속되어서, 결과적으로 모델이 예측을 수행하는 방법을 설명하기는 더 어려워질 수 있다.       
        

***         
         
           
마치며       
=============             
이번에는 모델의 입력Input 데이터를 준비하는 방식을 주로 다루었다.        
```        
  - 특징 해시: 카테고리 입력을 고유한 문자열로 인코딩       
  - 임베딩: 많은 카테고리 또는 텍스트 데이터가 있는 입력과 같이 카디널리티가 큰 데이터를 표현하는 기술        
  - 특징 교차: 두 특징을 결합하여 특징을 자체적으로 인코딩하고, 이를 통해 쉽게 포착할 수 없는 관계를 추출하는 접근 방식       
  - 멀티모달 입력: 서로 다른 유형의 입력을 동일한 모델에 결합하는 방법과 하나의 특징이 여러 방식으로 표현될 수 있는 방법에 대한 문제를 해결     
```       
다음에는 모델의 출력Output에 중점을 둔 패턴을 살펴볼 예정이다.      
